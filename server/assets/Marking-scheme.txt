# Drexel University
## College of Computing and Informatics
## INFO 212: Data Science Programming I
## Assignment 3 Answers and Marking Scheme

## Question 1 [20 marks]: Wikipedia has a Webpage: [List of countries by past and future population](https://en.wikipedia.org/wiki/List_of_countries_by_past_and_future_population). On this page, there are three tables containing the estimated populations from 1950 to 2050 for all the countries. Read the description on the Webpage to understand the information in the tables. Write Python code to use `requests` to read in the tables. Write programs to answer the following questions.
1. How many tables have you read in? List the columns of the DataFrames that contain the population data.
2.For each population DataFrame, rename the column names of average annual growth to meaningful and unique names.
3. Create a DataFrame containing all the data from the population tables. i.e., the DataFrame contains the popultation and average annual growth data from 1950-2050. You need to combine the three population tables. How many rows and columns in the final combined DataFrame?
4. For the top 10 most populous countries at 1950, plot their population data from 1950-2050 (in 5-year interval). The plot should use years 1950-2050 as x axis, and population numbers as y axis. The following figure shows the expected result.
5. For the same top 10 most populous contries at 1950, plot the average annual growth for each previous five-year period starting from 1955 to 2050 in 5-year interval. The following figure shows the expected result.
6. Map country names to their continents and add a 'continent' column storing the continent names corresponding to the countries. Plot the mean population and average annual growth over the years (1950-2050) for all continents.
7. What are the top 5 countries with the highest population growth rate from 1950 to 2020? How does this compare to their projected growth rates from 2020 to 2050? Discuss any notable differences and other factors that should be also considered together with the historical growth data to predict future population changes.

## Marking Schemes:

### -2 - -4: for reach wrong or missing answer.
### -5: If a plot doesn't show the information as requested.
### -2: for other issues
### -4: Discussion is missing for Question 7

# Solution:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
import requests
import seaborn as sns

url = "https://en.wikipedia.org/wiki/List_of_countries_by_past_and_projected_future_population"

html = requests.get(url)

tables = pd.read_html(html.text)

1. How many tables have you read in? List the columns of the DataFrames that contain the population data.


# The number of tables read in
len(tables)

tables[1].columns

tables[2].columns

tables[3].columns

2. For each population DataFrame, rename the column names of average annual growth to meaningful and unique names.

tables[1].columns = ['country', '1950', '1955', '1955AAG', '1960', '1960AAG',
       '1965', '1965AAG', '1970', '1970AAG', '1975', '1975AAG', '1980', '1980AAG']

tables[1].head()

tables[2].columns = ['country', '1985', '1985AAG', '1990', '1990AAG', '1995',
       '1995AAG', '2000', '2000AAG', '2005', '2005AAG', '2010', '2010AAG', '2015', '2015AAG']

tables[3].columns = ['country', '2020', '2020AAG', '2025', '2025AAG', '2030',
       '2030AAG', '2035', '2035AAG', '2040', '2040AAG', '2045', '2045AAG', '2050', '2050AAG']

3. Create a DataFrame containing all the data from the population tables. i.e., the DataFrame contains the popultation and average annual growth data from 1950-2050. You need to combine the three population tables. How many rows and columns in the final combined DataFrame?

# Create a DataFrame containing all the data from the population tables
# This has to be done by joining on country names
df1 = pd.merge(tables[1], tables[2], left_on="country", right_on="country")
df = pd.merge(df1, tables[3], left_on="country", right_on="country")

df.columns

df.shape

path = "/content/drive/MyDrive/Colab Notebooks/courses/INFO212/assignments/country_populations_1955_2050.csv"

df.to_csv(path, index=None)

4. For the top 10 most populous countries at 1950, plot their population data from 1950-2050 (in 5-year interval). The plot should use years 1950-2050 as x axis, and population numbers as y axis. The following figure shows the expected result: ![](https://i.imgur.com/6W9RuZO.png)

top_10 = df.sort_values(by='1950', ascending=False)[1:11]
top_10

top_10.columns

top_10_pop = top_10[['country', '1950', '1955', '1960', '1965', \
       '1970', '1975','1980', \
       '1985', '1990', '1995', '2000', \
       '2005', '2010', '2015', \
       '2020',  '2025', '2030', '2035', \
       '2040',  '2045',  '2050']]

top_10_pop.head()

# Set the country as the index
top_10_pop_country = top_10_pop.set_index('country')

# Switch row and column
top_10_pop_country = top_10_pop_country.stack().unstack(0)

top_10_pop_country.head(10)

top_10_pop_country.shape

years= top_10_pop_country.index

ax  = top_10_pop_country.plot.line(figsize=(12, 8), marker = 'o')
ax.set_xticks(range(21))
ax.set_xticklabels(years, rotation = 45)
ax.set_xlabel('Year')
ax.set_ylabel('Population in Thousands')
ax.legend(loc='best')

### Alternative Solution:

# Set the country as the index
top_10_pop_country_alt = top_10_pop.set_index('country')

top_10_pop_country_alt = top_10_pop_country_alt.T

top_10_pop_country_alt.head()

# For the top 10 most populous countries at 1950, plot
# their population data from 1950-2050 (in 5-year interval).
ax  = top_10_pop_country_alt.plot.line(figsize=(12, 8), marker = 'o')
ax.set_xticks(range(21))
ax.set_xticklabels(years, rotation = 45)
ax.set_xlabel('Year')
ax.set_ylabel('Population in Thousands')
ax.legend(loc='best')

5. For the same top 10 most populous contries at 1950, plot the average annual growth for each previous five-year period starting from 1955 to 2050 in 5-year interval. The following figure shows the expected result: ![](https://i.imgur.com/ImXjIo0.png)

top_10 = df.sort_values(by='1950', ascending=False)[1:11]
top_10

top_10_aag = top_10[['country', '1955AAG',
       '1960AAG', '1965AAG', '1970AAG',  '1975AAG',
        '1980AAG', '1985AAG', '1990AAG',
       '1995AAG', '2000AAG',  '2005AAG', '2010AAG',
        '2015AAG',  '2020AAG',  '2025AAG',
       '2030AAG',  '2035AAG',  '2040AAG',  '2045AAG',
        '2050AAG']]

top_10_aag.set_index('country', inplace = True)

top_10_aag = top_10_aag.T

top_10_aag.head()

# For the same top 10 most populous contries at 1950, plot
# the average annual growth for each previous five-year period
# starting from 1955 to 2050 in 5-year interval.
ax  = top_10_aag.plot.line(figsize=(12, 8), marker = 'o')
ax.set_xticks(range(20))
ax.set_xticklabels(years[1:], rotation = 45)
ax.set_xlabel('Year')
ax.set_ylabel('Average Annual Growth in Previous 5 Years (%)')
ax.legend(loc='best')

### Alternatively, the above transpose can be done using stack() and unstack() as in q4.

6. Map country names to their continents and add a 'continent' column storing the continent names corresponding to the countries. Plot the mean population and average annual growth over the years (1950-2050) for all continents.
- Hint: install the pycountry_convert package and use its methods for converting country names to continent names: https://pypi.org/project/pycountry-convert/

!pip install pycountry_convert

import pycountry_convert as cc

def get_continent(col):
    try:
        cn_a2_code =  cc.country_name_to_country_alpha2(col)
    except:
        cn_a2_code = 'Unknown'
    try:
        cn_continent = cc.country_alpha2_to_continent_code(cn_a2_code)
    except:
        cn_continent = 'Unknown'
    return cn_continent

conts = df.country.apply(get_continent)

conts

df.head(1)

cont_pops = df.iloc[:, 1:].groupby(conts).mean().iloc[:6]
cont_pops

pops = cont_pops[['1950', '1955', '1960', '1965', \
       '1970', '1975','1980', \
       '1985', '1990', '1995', '2000', \
       '2005', '2010', '2015', \
       '2020',  '2025', '2030', '2035', \
       '2040',  '2045',  '2050']]

ax  = pops.T.plot.line(figsize=(12, 8), marker = 'o')
ax.set_xticks(range(20))
ax.set_xticklabels(years[1:], rotation = 45)
ax.set_xlabel('Year')
ax.set_ylabel('Population in Thousands')
ax.legend(loc='best')

aags = cont_pops[['1955AAG',
       '1960AAG', '1965AAG', '1970AAG',  '1975AAG',
        '1980AAG', '1985AAG', '1990AAG',
       '1995AAG', '2000AAG',  '2005AAG', '2010AAG',
        '2015AAG',  '2020AAG',  '2025AAG',
       '2030AAG',  '2035AAG',  '2040AAG',  '2045AAG',
        '2050AAG']]

ax  = aags.T.plot.line(figsize=(12, 8), marker = 'o')
ax.set_xticks(range(20))
ax.set_xticklabels(years[1:], rotation = 45)
ax.set_xlabel('Year')
ax.set_ylabel('Average Annual Growth in Previous 5 Years')
ax.legend(loc='best')

7. What are the top 5 countries with the highest population growth rate from 1950 to 2020? How does this compare to their projected growth rates from 2020 to 2050? Discuss any notable differences and other factors that should be also considered together with the historical growth data to predict future population changes.

# Function to calculate growth rate over a period
def calculate_growth_rate(initial_population, final_population, periods):
    return (final_population / initial_population) ** (1 / periods) - 1

# Calculate the growth rate for each country from 1950 to 2020
df['1950_2020_GR'] = calculate_growth_rate(
    df['1950'],
    df['2020'],
    periods=(2020-1950) / 5  # Number of 5-year periods from 1950 to 2020
)

# Sort the countries by this growth rate
top_5_growth_1950_2020 = df[['country', '1950_2020_GR']].sort_values(by='1950_2020_GR', ascending=False).head(5)

top_5_growth_1950_2020

# Calculate the growth rate for each country from 2020 to 2050
df['2020_2050_GR'] = calculate_growth_rate(
    df['2020'],
    df['2050'],
    periods=(2050-2020) / 5  # Number of 5-year periods from 2020 to 2050
)

# Get the average growth rates the top 5 countries from 1950 to 2020
df[df.country.isin(top_5_growth_1950_2020.country)][['country', '1950_2020_GR', '2020_2050_GR']].sort_values(by='1950_2020_GR', ascending=False)

Discussion: Comparing the 1950-2020 growth rates to the projected growth rates from 2020 to 2050, we see a significant decrease. For instance, Qatar's growth rate is projected to slow to 0.76% over each 5-year period from 2020 to 2050. There could be multiple factors:

 - Fertility rates: A decline in fertility rates can lead to a lower growth rate.
 - Economic conditions: Economic prosperity or downturns can influence birth rates and migration patterns.
 - Environmental factors: Climate change, natural resources, and environmental sustainability can affect population distributions and growth.

Notable differences in growth rates may be due to transitions in these factors. For example, countries with high growth rates in the past might experience slower growth due to economic changes, shifts in government policies, or demographic transitions like aging populations.â€‹

## Question 2 [40 marks]: The link: https://yuan-json-api.vercel.app/nobelprize/laureates is a RESTful API endpoint that returns information in JSON about Nobel Prize Laureates. Write Python code to use `requests` to call the RESTful API and read the JSON results. Write program to answer the following questions.
1. Create a Pandas DataFrame containing the laureates information along with the information about their prizes and afflications in flat format. How many unique laureates are in the data set?

2. Develop two distinct methods to identify laureates who have been awarded Nobel prizes more than once, whether in different years or in different categories. Present your findings in a clear and organized manner. This could be displayed in a table format (like a pandas DataFrame).

 - Two separate and distinct approaches could be different algorithmic strategies, Python code, or data structures. For each approach, You need to provide a detailed explanation of the logic behind each method. The explanation should be in the form of comments within your code or as a separate written text accompanying your code.

3. Develop two distinct methods to list the top 5 afiliations by the number of awards. Show the laureates, prizes, and affiliations information. Present your findings in a clear and organized manner. This could be displayed in a table format (like a pandas DataFrame).
 - Two separate and distinct approaches could be using different algorithmic strategies, Python code, or data structures For each approach, you need to provide a detailed explanation of the logic behind each method. The explanation should be in the form of comments within your code or as a separate written text accompanying your code.

4. For the top 5 affiliations, develop two distinct visualizations to represent the counts of each Nobel prize category associated with these affiliations.

 - Two different types of plots could be bar chart, pie chart, line graph, etc. You also could choose to present the information in a single comprehensive figure or multiple figures for each affiliation.

# Marking Schemes:
## 1. Create a Pandas DataFrame containing the laureates information along with the information about their prizes and afflications in flat format. How many unique laureates are in the data set?
#### -4: If the student didn't show the answer and there is no coding effrot.
#### -2: If there is coding effort but only the answer is accidently wrong.
## 2. Develop two distinct methods to identify laureates who have been awarded Nobel prizes more than once, whether in different years or in different categories. Present your findings in a clear and organized manner. This could be displayed in a table format (like a pandas DataFrame).

 - Two separate and distinct approaches could be different algorithmic strategies, Python code, or data structures. For each approach, You need to provide a detailed explanation of the logic behind each method. The explanation should be in the form of comments within your code or as a separate written text accompanying your code.
#### -2: If the plot doesn't show the requested information but there is some coding effort.
#### -5: If the plot doesn't show the requested information. There is no coding effort.
#### -2: Other issues.

## 3. Develop two distinct methods to list the top 5 afiliations by the number of awards. Show the laureates, prizes, and affiliations information. Present your findings in a clear and organized manner. This could be displayed in a table format (like a pandas DataFrame).
 - Two separate and distinct approaches could be using different algorithmic strategies, Python code, or data structures For each approach, you need to provide a detailed explanation of the logic behind each method. The explanation should be in the form of comments within your code or as a separate written text accompanying your code.
#### -2: If the plot doesn't show the requested information but there is some coding effort.
#### -5: If the plot doesn't show the requested information. There is no coding effort.
#### -2: Other issues.

## 4. For the top 5 affiliations, develop two distinct visualizations to represent the counts of each Nobel prize category associated with these affiliations.

 - Two different types of plots could be bar chart, pie chart, line graph, etc. You also could choose to present the information in a single comprehensive figure or multiple figures for each affiliation.
#### -2: If the plot doesn't show the requested information but there is some coding effort.
#### -5: If the plot doesn't show the requested information. There is no coding effort.
#### -2: Other issues.

# Solution:

1. Create a Pandas DataFrame containing the laureates information along with the information about their prizes and afflications in flat format. How many unique laureates are in the data set?

# Write your code below
import pandas as pd
import numpy as np
import requests
import matplotlib.pyplot as plt

# URL of the RESTful API
url = "https://yuan-json-api.vercel.app/nobelprize/laureates"

# Make a GET request to the API
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Read the JSON results into a DataFrame
    data = response.json()

    # Assuming the JSON data is a list of records, which is common for APIs
    laureates = pd.json_normalize(data['laureates'])
    print(laureates.head())  # Print the first few rows of the DataFrame
else:
    print(f"Failed to retrieve data: {response.status_code}")

laureates.head()

laureates[laureates.id == '6'].prizes.values

laureates.shape

laureates.head()

data['laureates'][0]['prizes'][0]

# Expanding the 'prizes' column into a separate DataFrame
prizes = pd.json_normalize(laureates['prizes'].explode()).dropna(subset=['year']).reset_index(drop=True)

prizes.shape

prizes.id.nunique()

# join the prizes with laureates on their id columns:
laureates_prizes = pd.merge(laureates, prizes, left_on='id', right_on='id', how='left')

laureates_prizes = laureates_prizes.drop('prizes', axis=1)

laureates_prizes.shape

# Who had mulitple prizes
dup_ids = laureates_prizes[laureates_prizes.id.duplicated()].id
dup_ids

laureates_prizes.id.nunique()

# Initialize an empty list to collect the DataFrames
affiliations_frames = []

# Loop through each laureate and prize to normalize the affiliations
for index, row in laureates_prizes.iterrows():
    # If 'affiliations' is a non-empty list, we can normalize it
    if isinstance(row['affiliations'], list) and len(row['affiliations']) > 0:
        # Normalize the current row's affiliations
        current_affiliations = pd.json_normalize(row['affiliations'])
        # Add an identifier to match it back to the laureate and prize
        current_affiliations['affiliation_index'] = index
        # Append the current DataFrame to the list
        affiliations_frames.append(current_affiliations)

# Concatenate all the affiliation DataFrames into one DataFrame
affiliations_expanded = pd.concat(affiliations_frames, ignore_index=True)

affiliations_expanded.head()

# Now, let's merge the expanded affiliations back with the laureates and prizes data
# We drop the original 'affiliations' column since it's nested
laureates_prizes_expanded = laureates_prizes.drop('affiliations', axis=1)

# Merge the data on the 'affiliation_index' we added
final_df = pd.merge(
    laureates_prizes_expanded,
    affiliations_expanded,
    how="left",
    left_index=True,
    right_on="affiliation_index"
)

final_df = final_df.drop('affiliation_index', axis=1)

final_df.head()

# The number of unique laureates in the data
final_df.id.nunique()

2.Develop two distinct methods to identify laureates who have been awarded Nobel prizes more than once, whether in different years or in different categories. Present your findings in a clear and organized manner. This could be displayed in a table format (like a pandas DataFrame).

 - Two separate and distinct approaches could be different algorithmic strategies, Python code, or data structures. For each approach, You need to provide a detailed explanation of the logic behind each method. The explanation should be in the form of comments within your code or as a separate written text accompanying your code.


# Who had mulitple prizes
dup_ids = laureates_prizes[laureates_prizes.id.duplicated()].id
dup_ids.values

final_df[final_df.id.isin(dup_ids)][['id', 'firstname', 'surname', 'year', 'category']]

3. Develop two distinct methods to list the top 5 afiliations by the number of awards. Show the laureates, prizes, and affiliations information. Present your findings in a clear and organized manner. This could be displayed in a table format (like a pandas DataFrame).
 - Two separate and distinct approaches could be using different algorithmic strategies, Python code, or data structures For each approach, you need to provide a detailed explanation of the logic behind each method. The explanation should be in the form of comments within your code or as a separate written text accompanying your code.

# With the final_df DataFrame restored, we can now proceed to list the top 5 affiliations by the number of awards

# Count the number of awards per affiliation name
affiliation_award_counts = final_df['name'].value_counts().head(5)

affiliation_award_counts

# Get the top 5 affiliation names
top_affiliations = affiliation_award_counts.index.tolist()

# Filter the final_df DataFrame for rows that have these top affiliations
top_affiliation_data = final_df[final_df['name'].isin(top_affiliations)]

# Select relevant columns to display laureates, prizes, and affiliation information
top_affiliation_info = top_affiliation_data[['id', 'firstname', 'surname', 'year', 'category', 'name', 'city', 'country']]

# Display the top 5 affiliations with the most number of awards and the associated laureates and prizes
top_affiliation_info.sort_values(by='name')

4. For the top 5 affiliations, develop two distinct visualizations to represent the counts of each Nobel prize category associated with these affiliations.

 - Two different types of plots could be bar chart, pie chart, line graph, etc. You also could choose to present the information in a single comprehensive figure or multiple figures for each affiliation.

# Filter final_df for only the top 5 affiliations
top_affiliation_prizes = final_df[final_df['name'].isin(top_affiliations)]

# Count the occurrences of each prize category within the top affiliations
top_affiliation_category_counts = top_affiliation_info.groupby(['name', 'category']).size().unstack(fill_value=0)

# Plot the counts of each category for the top 5 affiliations
top_affiliation_category_counts.plot(kind='bar', stacked=True, figsize=(14, 7), title='Nobel Prize Category Counts for Top 5 Affiliations')


## Question 3 [20 marks]: The file `weather-samples.csv` contains weather data measured at a weather station for a period of three years. Sensors at the weather station capture weather-related measurements such as air temperature, air pressure, and relative humidity. Load  the data to a pandas DataFrame. Write code to answer the following questions.

1. How many rows contain missing values?
2. How many columns contain missing values?
3. List the numbers of missing values for all columns.
4. For each missing value, fill up it with the mean value of the column where the missing value is located in.
5. Show there is no missing value after filling up.
6. Plot the histogram of the value `relative_humidity_3pm`.
7. A weather forecast problem is to predict a measure in a future time using the measurements at earlier moments. For example, the data set contains several measurements at 9am and one measurement `relative_humidity_3pm` at 3pm. It is interesting to know whether we can predict `relative_humidity_3pm` using the values at 9am. This is a typical supervised machine learning problem. Using plots to visualize the relationships between the measurements at 9am and `relative_humidity_3pm`. What do you find? Discuss the impacts of the measurements at 9am to the values of `relaive_humidity_3pm`.

# Marking Schema:
## -2: for each incorrect or incomplete answer. In particular, they must use programs to answer the questions.
## -4: If there is no discussion about the correlations between the measurements at 9am (there are 9 measures) and 'relative_humidity_3pm'. Just showing the plots is not enough.


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from google.colab import files
files.upload()

weather = pd.read_csv("weather-samples.csv", index_col=0)

weather.columns

1. How many rows contain missing values?

# the number of rows containing missing values is:
(weather.isna().sum(axis=1) > 0).sum()

2. How many columns contain missing values?

# The number of columns containing missing values is:
(weather.isna().sum(axis=0) > 0).sum()

3. List the numbers of missing values for all columns.

weather.isna().sum()

4. For each missing value, fill up it with the mean value of the column where the missing value is located in.

weather_filled = weather.fillna(weather.mean())

weather_filled.head()

5. Show there is no missing value after filling up.

# The number of missing values in the dataset is:
weather_filled.isna().sum().sum()

6. Plot the histogram of the value `relative_humidity_3pm`.

plt.hist(weather['relative_humidity_3pm'])

7.A weather forecast problem is to predict a measure in a future time using the measurements at earlier moments. For example, the data set contains several measurements at 9am and one measurement `relative_humidity_3pm` at 3pm. It is interesting to know whether we can predict `relative_humidity_3pm` using the values at 9am. This is a typical supervised machine learning problem. Using plots to visualize the relationships between the measurements at 9am and `relative_humidity_3pm`. What do you find? Discuss the impacts of the measurements at 9am to the values of `relaive_humidity_3pm`.

cols = ['air_pressure_9am', 'air_temp_9am', 'avg_wind_direction_9am',
       'avg_wind_speed_9am', 'max_wind_direction_9am', 'max_wind_speed_9am',
       'rain_accumulation_9am', 'rain_duration_9am', 'relative_humidity_9am']

fig, axes = plt.subplots(3, 3, figsize=(15, 9))
for r in np.arange(3):
    for c in np.arange(3):
        depent = cols[3 * r + c]
        sns.regplot(ax=axes[r, c], x=depent, y='relative_humidity_3pm', data=weather)
plt.tight_layout()

# Discussion:
Discuss the correlations between every measure at 9am and the target variable 'relative_humidity_3pm'. Explain which measures have high impacts on the target variable. Which measure would be a good predictor for the target variable.



## Question 4 [20 marks]: Download the dataset: `activity-data.csv` and load it into a data frame. The data set contains the time series outputs of a wearable clinical device measuring a patient's activities. Write code to answer the following questions:
1. What is the total number of days covered by the dataset? Provide a list of all the distinct calendar dates on which recordings were made.
2. Can you plot the number of steps against the recording 'Datetime', ensuring that each tick on the x-axis corresponds to a unique date in the dataset?
3. Create a plot that shows the daily average number of steps in comparison with the steps recorded each minute. The plot should clearly differentiate between the daily mean and individual minute measurements.
4. How does the average step count change during 15-minute intervals throughout the day? Compute and plot these averages across all days in the dataset.
5. For each day represented in the dataset, what is the percentage of minutes without records (missing records)? Please calculate and present this data
6. For minutes with missing data, fill in 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories' using the mean values for those minutes across all days where data is available. Save the completed dataset to a DataFrame named `all_activity_df` with columns for 'Datetime', 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories'.
7. Could you create plots that illustrate the pairwise relationships between 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories'? After analyzing these plots, what conclusions can you draw about the relationships and potential correlations between these variables?

## Marking Schemes
### 1. What is the total number of days covered by the dataset? Provide a list of all the distinct calendar dates on which recordings were made.
#### -2: for a missing answer

### 2. Can you plot the number of steps against the recording 'Datetime', ensuring that each tick on the x-axis corresponds to a unique date in the dataset?

#### -4: for missing or wrong plot

### 3. Create a plot that shows the daily average number of steps in comparison with the steps recorded each minute. The plot should clearly differentiate between the daily mean and individual minute measurements.

#### -2: incorrect plot

### 4. How does the average step count change during 15-minute intervals throughout the day? Compute and plot these averages across all days in the dataset.

#### -2: wrong values for mean steps for every 15 minutes over all days.
#### -2: for missing plot

### 5. For each day represented in the dataset, what is the percentage of minutes without records (missing records)? Please calculate and present this data

#### -2: missing percentage

### 6. For minutes with missing data, fill in 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories' using the mean values for those minutes across all days where data is available. Save the completed dataset to a DataFrame named `all_activity_df` with columns for 'Datetime', 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories'.

#### -2: didn't fill up the missing values with the correct mean values.

### 7. Could you create plots that illustrate the pairwise relationships between 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories'? After analyzing these plots, what conclusions can you draw about the relationships and potential correlations between these variables?
#### -2: missing pair-wise plots
#### -2: discussion is missing


Load and Overview the Data

path = "/content/drive/MyDrive/Colab Notebooks/courses/INFO212/assignments/patients_data/activity-data.csv"

df = pd.read_csv(path, parse_dates=['Datetime'])

df.head()

df.Steps.max(), df.Steps.min()

df.info()

df.shape

df.columns

1. What is the total number of days covered by the dataset? Provide a list of all the distinct calendar dates on which recordings were made.

# The number of days the recordings span is
len(df.Day.unique())

# The list of unique calendar dates in the recording time:
df.Datetime.dt.date.unique()

2. Can you plot the number of steps against the recording 'Datetime', ensuring that each tick on the x-axis corresponds to a unique date in the dataset?

import matplotlib.dates as mdates

# Sort your data based on 'Datetime'
df.sort_values('Datetime', inplace=True)

# Create a plot
plt.figure(figsize=(10,6))  # Adjust size as per need
plt.plot(df['Datetime'], df['Steps'])
plt.title('Steps over Time')
plt.xlabel('Datetime')
plt.ylabel('Steps')

# Adjust x-ticks
ax = plt.gca()
ax.xaxis.set_major_locator(mdates.DayLocator())  # display a tick for each day
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # format the date as 'YYYY-MM-DD'
plt.xticks(rotation=45, ha='right')  # rotate x-ticks 45 degrees


plt.show()

3. Create a plot that shows the daily average number of steps in comparison with the steps recorded each minute. The plot should clearly differentiate between the daily mean and individual minute measurements.

import matplotlib.pyplot as plt
import matplotlib.dates as mdates

# Ensure that 'Datetime' is in the correct datetime format
df['Datetime'] = pd.to_datetime(df['Datetime'])

# Sort your data based on 'Datetime'
df.sort_values('Datetime', inplace=True)

# Compute the daily mean steps
df_daily_mean = df.set_index('Datetime').resample('D')['Steps'].mean()

# Create a plot
fig, ax = plt.subplots(figsize=(10,6))  # Adjust size as per need

# Plot the per-minute steps
ax.plot(df['Datetime'], df['Steps'], label='Per-Minute Steps')

# Plot the daily mean steps
ax.plot(df_daily_mean.index, df_daily_mean, label='Daily Mean Steps', color='red')

ax.set_title('Steps over Time')
ax.set_xlabel('Datetime')
ax.set_ylabel('Steps')

# Adjust x-ticks
ax.xaxis.set_major_locator(mdates.DayLocator())  # display a tick for each day
ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))  # format the date as 'YYYY-MM-DD'
plt.xticks(rotation=45, ha='right')  # rotate x-ticks 45 degrees and align to the right

plt.legend()  # Display the legend

plt.show()


4. How does the average step count change during 15-minute intervals throughout the day? Compute and plot these averages across all days in the dataset.

import matplotlib.pyplot as plt
import matplotlib.dates as mdates

df_index = df.set_index('Datetime')

# Group by time and compute the mean steps for each minute
df_minute_mean = df.groupby(df_index.index.time)['Steps'].mean()

# Convert the index to a DatetimeIndex
df_minute_mean.index = pd.to_datetime(df_minute_mean.index, format='%H:%M:%S')

# Resample to compute mean steps every 15 minutes
df_15min_mean = df_minute_mean.resample('15T').mean()

# Create a plot
fig, ax = plt.subplots(figsize=(10,6))  # Adjust size as per need
ax.plot(df_15min_mean.index, df_15min_mean, label='15-Minute Mean Steps')

ax.set_title('15-Minute Mean Steps over All Days')
ax.set_xlabel('Time')
ax.set_ylabel('Steps')

# Set x-ticks to display every hour
ax.xaxis.set_major_locator(mdates.HourLocator(interval=1))

# Format x-ticks as 'HH:MM'
ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))

plt.xticks(rotation=45, ha='right')  # rotate x-ticks 45 degrees and align to the right

plt.legend()  # Display the legend

plt.show()


5. For each day represented in the dataset, what is the percentage of minutes without records (missing records)? Please calculate and present this data


# Extract all unique dates
unique_dates = df['Datetime'].dt.date.unique()
unique_dates

df.groupby(df.Datetime.dt.date)['Datetime'].count()

# Compute the percentage of missing minute recordings for each day
non_missing_percentage = (df.groupby(df.Datetime.dt.date)['Datetime'].count() / 1440) * 100

print(non_missing_percentage)

## The missing percentage is 1 - non_missing_percentage

6. For minutes with missing data, fill in 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories' using the mean values for those minutes across all days where data is available. Save the completed dataset to a DataFrame named `all_activity_df` with columns for 'Datetime', 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories'.


df.columns

df_means = df[['Datetime', 'Steps', 'XCnt', 'YCnt', 'ZCnt', 'Calories']].copy()

# Extract the minute from each timestamp in df_means
df_means['Minute'] = df_means['Datetime'].dt.strftime('%H:%M')
df_means

df_means = df_means.groupby('Minute').mean()

df_means

df_means['Steps'].plot.line()

# Extract all unique dates
unique_dates = df['Datetime'].dt.date.unique()
unique_dates

# Create an empty list to hold all datetime ranges
all_datetimes = []

# For each unique date, generate a complete datetime range for each minute of the day
for date in unique_dates:
    day_start = pd.to_datetime(date)
    day_end = day_start + pd.DateOffset(days=1) - pd.DateOffset(minutes=1)  # up to 23:59 of the current day
    datetimes = pd.date_range(day_start, day_end, freq='1min')
    all_datetimes.extend(datetimes)

# Convert the list of datetime ranges to a pandas Series or DataFrame if necessary
all_datetimes_series = pd.Series(all_datetimes)

all_datetimes_df = pd.DataFrame(all_datetimes_series, columns=['Datetime'])

# Perform a left join on 'Datetime'
merged_df = pd.merge(all_datetimes_df, df, on='Datetime', how='left')

all_activity = merged_df[['Datetime', 'Steps', 'XCnt', 'YCnt', 'ZCnt', 'Calories']].copy()

all_activity

# Extract the minute from each timestamp in all_activity
all_activity['Minute'] = all_activity['Datetime'].dt.strftime('%H:%M')
all_activity

# For each missing value in 'Steps' and 'XCnt', replace it with the corresponding mean value from df_means
all_activity['Steps'] = all_activity.apply(lambda row: df_means.loc[row['Minute'], 'Steps'] if pd.isnull(row['Steps']) else row['Steps'], axis=1)

all_activity

# Similarly, fill up XCnt, YCnt, ZCnt, and Calories
all_activity['XCnt'] = all_activity.apply(lambda row: df_means.loc[row['Minute'], 'XCnt'] if pd.isnull(row['XCnt']) else row['XCnt'], axis=1)
all_activity['YCnt'] = all_activity.apply(lambda row: df_means.loc[row['Minute'], 'YCnt'] if pd.isnull(row['YCnt']) else row['YCnt'], axis=1)
all_activity['ZCnt'] = all_activity.apply(lambda row: df_means.loc[row['Minute'], 'ZCnt'] if pd.isnull(row['ZCnt']) else row['ZCnt'], axis=1)
all_activity['Calories'] = all_activity.apply(lambda row: df_means.loc[row['Minute'], 'Calories'] if pd.isnull(row['Calories']) else row['Calories'], axis=1)

all_activity = all_activity.drop('Minute', axis=1)
all_activity

7. Could you create plots that illustrate the pairwise relationships between 'Steps', 'XCnt', 'YCnt', 'ZCnt', and 'Calories'? After analyzing these plots, what conclusions can you draw about the relationships and potential correlations between these variables?

sns.pairplot(all_activity[['Steps', 'XCnt', 'YCnt', 'ZCnt', 'Calories']])

## Discussion: Insights....

